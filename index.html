<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="author" content="">

<title>CS 152 - Image Style Learning</title>

<!-- - Bootstrap Core CSS -->
<link href="css/bootstrap.min.css" rel="stylesheet">

<!-- Custom CSS -->
<link href="css/full-width-pics.css" rel="stylesheet">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<!-- github button -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- font -->
<link href='css/Source-Sans-Pro.css' rel='stylesheet' type='text/css'>
</head>

<body>
<!-- Full Width Image Header with Logo -->
<!-- Image backgrounds are set within the full-width-pics.css file. -->
<header class="image-bg-fluid-height">
  <!-- <img class="img-responsive img-center" src="http://placehold.it/200x200&text=Logo" alt="">-->
</header>

<!-- Content Section -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-lg-9"></div>
      <div class="col-lg-1">
        <a class="github-button" href="https://github.com/mjenrungrot/hmc-cs152/" data-icon="octicon-eye" data-style="mega" aria-label="Watch mjenrungrot/hmc-cs152 on GitHub">Watch</a>
      </div>
      <div class="col-lg-1">
        <a class="github-button" href="https://github.com/mjenrungrot/hmc-cs152/fork" data-icon="octicon-repo-forked" data-style="mega" aria-label="Fork mjenrungrot/hmc-cs152 on GitHub">Fork</a>
      </div>
      <div class="col-lg-1">
        <a class="github-button" href="https://github.com/mjenrungrot/hmc-cs152/archive/master.zip" data-icon="octicon-cloud-download" data-style="mega" aria-label="Download mjenrungrot/hmc-cs152 on GitHub">Download</a>
      </div>
    </div>
    <div class="row">
      <div class="col-lg-12">
        <h1 class="section-heading">CS152 - Neural Network Final Project</h1>
        <h1 class="section-heading">Introduction</h1>
        <p class="lead section-lead">Image Style Learning</p>
        <p class="section-paragraph">
          Style transfer problem has become very popular in the field of computer vision and image processing.
          As introduced by <cite><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">
          Image style transfer using convolutional neural networks</a></cite>, the authors have shown that using image representations derived
          from convolutional neural networks can explicitly represent semantic information and is able to separate image content from image style.
          The authors demonstrated their works by introducing a neural algorithm for constructing an image that has similar content to a reference
          content image and similar style to a reference style image. This idea of representing an image with deep neural networks, as demonstrated in <cite><a href="https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf">
          Understanding deep image representations by inverting them</a></cite>, has been shown that several layers of deep convolutional neural
          networks can retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.
        </p>
        <p class="section-paragraph">
        Further using an idea of representing an image with deep convolutional neural networks, we are interested in the problem that, given set of images
        known to be constructed according to neural algorithm, presented by <cite>Image style transfer using convolutional neural networks</cite>, how
        can one generate a style image that is generalizable to be used to transform any image into a new image that has similar style to the reference images
        used to construct a style image. From <a href="#Figure1">Figure 1</a>, we are interested in developing a neural algorithm that can learn how to transform an image from
        the left to an image from the right, given we have a dataset of images similar to the one the right.
        </p>
        <p class="section-paragraph">
        The scope of this final project is to generate a style image that can be applied to any content image using the neural algorithm
        explained in <cite>Image style transfer using convolutional neural networks</cite> such that the generated, produced image is
        artistically comparable to an image transformed by existing tools EverFilter, which is used to artistically change the style of an input
        image. <a href="#Figure1">Figure 1</a> shows an example of images generated by EverFilter.
        </p>
        <center id="Figure1">
          <img src="img/example1_original.jpeg" width="42.5%">
          <img src="img/example1_filtered.jpeg" width="42.5%"><br>
          <div style="text-align:center;padding-left:8%; padding-right:8%;">
            Figure 1: A comparison between the reference content image and an image generated by the algorithm. The right image is generated by
            EverFilter.
          </div>
        </center>
      </div>
    </div>
    <!-- /.row -->
  </div>
  <!-- /.container -->
</section>

<section>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1 class="section-heading">Previous Works</h1>
        <p class="lead section-lead"><cite>Image style transfer using convolutional neural networks</cite></p>
        <p class="section-paragraph">
          In transferring style problem, the goal is to synthesize a new image that contains both semantic content information of the reference
          content image and artistic style information of the reference style image. As demonstrated in <a href="#Figure2">Figure 2</a>, an image
          is generated based on a reference content image depicting the Tübinger, Germany, and a reference style image <i>The Shipwreck of the
          Minotaur</i> by J.M.W. Turner, 1805. The generated image is able to illustrate the content information similar to the reference
          content image of the Tübinge while also render the style information similar to the reference style image <i>The Shipwreck of the
          Minotaur</i>.
        </p>
        <center id="Figure2">
          <img src="img/example_style_transfer.png" width="85%"><br>
          <div style="text-align:left;padding-left:10%; padding-right:10%;">
            Figure 2: an example of results produced by a neural algorithm described in <cite>Image style transfer using convolutional neural networks</cite>.
            Left: a content image depicting the Neckarfront in Tübinger, Germany;
            Right: a transformed image generated by the algorithm;
            Bottom: the painting named <i>The Shipwreck of the Minotaur</i> by J.M.W. Turner, 1805.
            This example is taken from <cite>Image style transfer using convolutional neural networks</cite>.
          </div>
        </center>
        <p class="section-paragraph">
          The definition of style and content of an image is ambiguous. Because we do not have exact definitions of what portions of an image
          contributes to "style" or "content" of an image. These two terms are also vaguely defined, depending on individuals. Before moving forward,
          we define a few definitions regarding style and content information of an image.
        </p>
        <p class="section-paragraph">
          Suppose $\vec{p}$ represent a content image vector, $\vec{s}$ represent a style image vector, and $\vec{x}$ represent any input
          image vector. Recall that the algorithm involves representing an image using layers of convolutional neural networks. Suppose
          layers used to represent a style image is $\mathcal{S}$ and layers used to represent a content image is $\mathcal{C}$.
        </p>
        <p class="section-paragraph">
          <u><b>Definition 1</b></u>: Let $P^l$ and $X^l$ be representations of a content image $\vec{p}$ and an input image $\vec{x}$ on layer $l$
          of a trained network. Then, two images have similar content information if their high-level features by the trained classifier have small
          Euclidean distance. In other words, two images have similar style if the following loss function regarding content representation
          of two images is small.
        </p>
        <center class="math-equation-center">
          $\mathcal{L}_\text{content}(\vec{p},\vec{x}) = \sum\limits_{l\in\mathcal{C}} \left[ \frac{1}{U_l} \sum\limits_{i,j} \left(P_{ij}^{l} - X_{ij}^{l}\right)^2 \right]$
        </center>
        <p class="section-paragraph">
          where $X_{ij}^l$ and $P_{ij}^{l}$ are the feature representations on layer $l$ of $i^\text{th}$ filter at position $j$
          of an input image and a content image, respectively, as represented by the trained classifier. The term $U_l$ is the total number of
          units on layer $l$, used for the purpose of weighting contribution of each layer.
        </p>
        <p class="section-paragraph">
          <u><b>Definition 2</b></u>: Let $S^l$ and $X^l$ be representations of a style image $\vec{s}$ and an input image $\vec{x}$ on layer $l$
          of a trained network. Two images have similar style if the difference between their features' Gram matrices has a small Frobenius norm.
          The Gram matrix, $G_\vec{x}^l$, of any image $\vec{x}$ on layer $l$ is defined as follows:
        </p>
        <center class="math-equation-center">
          $G_\vec{x}^l = \left[ G_{\vec{x},ij}^l \right]$
        </center>
        <p class="section-paragraph">
          where each element of the Gram matrix is given by inner products between feature representations on the trained classifier on
          layer $l$ for any two different filters:
        </p>
        <center class="math-equation-center">
          $G_{\vec{x},ij}^{l} = \sum\limits_{k}X^{l}_{ik}X_{jk}^l$.
        </center>
        <p class="section-paragraph">
          In the same way, two images have similar style if the following loss function regarding style representation of two images is small.
        </p>
        <center class="math-equation-center">
          $\mathcal{L}_\text{style}\left(\vec{s},\vec{x}\right) = \sum\limits_{l \in \mathcal{S}} \left[ \frac{1}{U_l} \sum\limits_{i,j}\left( G_{\vec{s},ij}^l - G_{\vec{x},ij}^l \right)^2 \right]$
        </center>
        <p class="section-paragraph">
          where $G_{\vec{s},ij}^l$ and $G_{\vec{x},ij}^l$ are elements at $i^\text{th}$ row
          and $j^\text{th}$ column of Gram matrices for a style image $\vec{s}$ and an input image $\vec{x}$
          based on feature representations in the trained classifier on layer $l$.
        </p>
        <p class="lead section-lead">Algorithm</p>
        <p class="section-paragraph">
          In <cite>Image style transfer using convolutional neural networks</cite>, a deep convolutional neural network (VGG-19) is used as a trained
          classifier for obtaining feature representation of any input images, content images, and style images. VGG-19 network is a classifier trained
          for solving image recognition task on over 10 millions images. The set of layers used for representing content information is
          $\mathcal{C} = \{\texttt{conv4_2}\}$. The set of layers used for representing style information is $\mathcal{S} = \{\texttt{conv2_1}, \texttt{conv3_1}, \texttt{conv4_1}, \texttt{conv5_1}\}$.
        </p>
        <p class="section-paragraph">
          More details about the structure of VGG network is available at <cite><a href="https://arxiv.org/pdf/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></cite>.
          The main advantages of using the VGG network, compared to other convolutional neural network, are 1) able to run on a large scale image recognition settings, 2) generalizable with different
          datasets, and 3) representable with deep networks. The generalizability over different kind of datsets and its deep network structure make
          it possible for each layer to retain photographically accurate information, with different degree of variation. Specifically, the deeper
          layer of the network is responsible for global structure of an image, while the shallower layer of the network is responsible for
          fine structure of an image, such as strokes or edges. Note that the VGG network is the winner of the
          <a href="http://www.image-net.org/challenges/LSVRC/2012/">ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012)</a>.
          The dataset ImageNet contains over 10 millions of images with various kind of objects.
        </p>
        <p class="section-paragraph">
          Given a reference style image $\vec{s}$ and a reference content image $\vec{p}$, the algorithm starts by putting these two images into the
          trained classifier VGG-19 network in order to obtain feature representations on each layer of the VGG-19 network. For style features,
          the Gram matrix on each layer $l \in \mathcal{S} = \{\texttt{conv2_1}, \texttt{conv3_1}, \texttt{conv4_1}, \texttt{conv5_1}\}$ is calculated and stored.
          For content features, the feature representation on layer $l \in \{\texttt{conv4_2}\}$ is stored. Once we have all feature representations
          of reference images, we then try to generate an image $\vec{x}$ that minimizes the following loss function:
        </p>
        <center class="math-equation-center">
          $\mathcal{L}_\text{total}(\vec{p},\vec{s},\vec{x}) = \alpha \mathcal{L}_\text{content}(\vec{p},\vec{x}) + \beta \mathcal{L}_\text{style}(\vec{s},\vec{x})$
        </center>
        <p class="section-paragraph">
          where $\alpha$ and $\beta$ are weighting factors for content and style reconstruction, respectively. The learning algorithm will then optimize $\vec{x}$ such
          that it minimizes $\mathcal{L}_\text{total}(\vec{p},\vec{s},\vec{x})$ using gradient descent algorithm, such as L-BFGS. In the process of optimizing loss function, we start with a randomized white noise. This randomized white noise is put into a trained
          classifier VGG-19 network to obtain feature representations in each layer of the network. The Gram matrix is also calculated accordingly
          in the same manner. Next, the loss function is calculated. Since the loss function is differentiable, we then follow the gradient descent
          algorithm to optimize an input image $\vec{x}$ that optimizes the total loss function $\mathcal{L}_\text{total}(\vec{p},\vec{s},\vec{x})$.
        </p>

      </div>
    </div>
    <!-- /.row -->
  </div>
  <!-- /.container -->
</section>

<section>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1 class="section-heading">Animated Style Learning</h1>
        <p class="lead section-lead">Background</p>
        <p class="section-paragraph">
          In this final project, we aim to construct an algorithm that construct a style image
        </p>
        <center>
          <img src="img/example1_original.jpeg" width="42.5%">
          <img src="img/example1_filtered.jpeg" width="42.5%"><br>
          <div style="text-align:center;padding-left:10%; padding-right:10%;">
            Figure 2: Comparison between the original version and the expected version that the network will learn
          </div>
        </center>
        <p class="lead section-lead">Approach</p>
        <p class="section-paragraph">
        In this section, we describe our approach on constructing a style image. We'll follow the definition of a similarity between two images' styles according to
        <cite>Image style transfer using convolutional neural networks</cite>. That is, two images have similar style if and only if the difference between
        features' Gram matrices has a small Frobenius norm. Here, we re-modeled the objective function so that it excluded the cost function due to content loss.
        The justification is that content information should take no effect in obtaining style information. Using a similar method of image reconstruction, we
        start with a randomized white noise and try to optimize the cost function using backpropagation to create a style image.
        </p>
        <p class="section-paragraph">
        As the filter must be generalizable across various kind of images, we need to further construct a style image based on multiple transformed images. In this
        project, we sampled the transformed images manually from online sites. The objective function of our approach becomes
        </p>
          <center class="math-equation-center">
            $\mathcal{L}(\texttt{dataset},\vec{x}) = \sum\limits_{i=1; \vec{s} \in \texttt{dataset}}^{|\texttt{dataset}|}\alpha_{i}\mathcal{L}_\text{style}(\vec{s},\vec{x})$
          </center>
          <p class="section-paragraph">

          where $\vec{s}$ and $\vec{x}$ is a style image vector and an input image vector, and $\alpha_i$ is a weighting parameter of each reference style image.
        </p>
        <center>
          <img src="img/proposed_algorithm.png" width=65%><br>
          <div style="text-align:left;padding-left:10%; padding-right:10%;">
          Figure 3: The structure of our proposed algorithm. At every iteration, we randomize images from the dataset, then input to the VGG-19 pre-trained network.
          We then calculate the Gram's matrix, F1, from the result of pre-trained network. To create a style image, we input to same VGG-19 pre-trained network in order to compute the Gram's matrix, F2,
          in the similar manner. We then update the style image using the back-propagation method to minimize the Frobenius norm between F1 and F2.
          </div>
        </center>
        <p class="section-paragraph">
        The workflow of our approach is depicted in Figure 3. We obtain the pre-trained model of VGG-19 model. Next, we applied the pre-trained VGG-19 model on
        all images in the dataset and store them to be used during optimization process. Then, during the optimization process, a white noise image gets passed
        to the pre-trained VGG-19 model. Encodings on all layers of the pre-trained VGG-19 model are extracted to compute Gram's matrices, shown as F1 and F2 in
        Figure 3. Then, the objective function is calculated accordingly. The optimization process is an iterative algorithm and continues for a certain number
        of iterations.
        </p>

        <p class="section-paragraph">
        As one might notice in Figure 2 that the background is very different from the one in original picture, we suspect that Everfilter also did some sort
        of image segmentation algorithm and applied the neural algorithm only on the foreground part of the image. To resolve this issue, we decided to use
        to use a mask to differentiate information about an image's foreground and its background. We discuss this approach later in the next section.
        </p>
        </div>
        <!-- /.row -->
    </div>
    <!-- /.container -->
</section>

<section>
    <div class="container">
        <div class="row">
            <div class="col-lg-12">
                <h1 class="section-heading">Results & Discussion</h1>
                <p class="lead section-lead">1st Attempt</p>
                <p class="section-paragraph">
                In this project, we ran all the testings and optimization on the Pittsburgh Supercomputing Center, which is a high performance computing center.
                The specification of a GPU node I've been using is HPE Apollo 2000s, each with 2 NVIDIA P100 GPUs, 2 Intel Xeon E5-2683 v4 CPUs (16 cores per CPU),
                128GB RAM, and 8TB on-node storage. The NVIDIA P100 GPU has Memory Clock 1.4Gbps and VRAM 12GB.
                </p>

                <p class="section-paragraph">
                First, we tested our approach with a dataset of size one. Figure 4, Figure 5, and Figure 6 show examples of output from our
                proposed algorithm. The training process takes roughly about 30 seconds for each epoch, which yields approximately 13 hours
                for generating a style image. Here, we include the generated images at different iterations.
                </p>
                <center style="padding-bottom:35px;">
                    <img src="img/everfilter/1.jpg", width="40%">
                    <img src="img/db1-everfilter1/everfilter1_at_iteration_15.png", width="40%"><br>
                    <img src="img/db1-everfilter1/everfilter1_at_iteration_150.png", width="40%">
                    <img src="img/db1-everfilter1/everfilter1_at_iteration_1500.png", width="40%">
                    <br>
                    <div style="text-align:left;padding-left:10%; padding-right:10%;">
                    Figure 4: An example of output from the algorithm for the dataset of size one. Top-left: the reference transformed image;
                    Top-right: the generated image after 15 epochs; Bottom-left: the generated image after 150 epochs; Bottom-right:
                    the generated image after 1500 epochs.
                    </div>
                </center>
                <center style="padding-bottom:35px;">
                    <img src="img/everfilter/13.jpg", width="40%">
                    <img src="img/db1-everfilter13/everfilter13_at_iteration_15.png", width="40%"><br>
                    <img src="img/db1-everfilter13/everfilter13_at_iteration_150.png", width="40%">
                    <img src="img/db1-everfilter13/everfilter13_at_iteration_1500.png", width="40%">
                    <br>
                    <div style="text-align:left;padding-left:10%; padding-right:10%;">
                    Figure 5: Another example of output from the algorithm for the dataset of size one. Top-left: the reference transformed image;
                    Top-right: the generated image after 15 epochs; Bottom-left: the generated image after 150 epochs; Bottom-right:
                    the generated image after 1500 epochs.
                    </div>
                </center>
                <center style="padding-bottom:35px;">
                    <img src="img/everfilter/4.jpg", width="40%">
                    <img src="img/db1-everfilter4/everfilter4_at_iteration_15.png", width="40%"><br>
                    <img src="img/db1-everfilter4/everfilter4_at_iteration_150.png", width="40%">
                    <img src="img/db1-everfilter4/everfilter4_at_iteration_1500.png", width="40%">
                    <br>
                    <div style="text-align:left;padding-left:10%; padding-right:10%;">
                    Figure 6: Another example of output from the algorithm for the dataset of size one. Top-left: the reference transformed image;
                    Top-right: the generated image after 15 epochs; Bottom-left: the generated image after 150 epochs; Bottom-right:
                    the generated image after 1500 epochs.
                    </div>
                </center>
                <center style="padding-bottom:35px;">
                    <iframe width="900" height="800" frameborder="0" scrolling="no" src="https://plot.ly/~mjenrungrot12f5/7.embed?share_key=twLhRfnmQU9EWYLvh60NvH"></iframe>
                    <div style="text-align:center;padding-left:10%; padding-right:10%;">
                    Figure 7: The learning curve for 3 examples shown above.
                    </div>
                </center>
                <p class="section-paragraph">
                The results shown by Figures 5-7 are very interesting, as the generated style images seem to be able to capture semantic content information
                of the reference images. As one may notice, the generated images at 1500 epochs seem to be able to provide correct colors composition that
                matches their reference images. Consider Figure 4 and Figure 6 as an example. The sky portion of generated images have different tone of colors,
                according to their own original reference images.

                As predicted earlier, we see that the background portion and foreground portion of generated images are jumbled together.
                This makes sense because if we believe that the original reference images were created based on one style image, then that style image
                should be generalizable among all reference images. To address this issue, we will run another experiment with a dataset of size 4. Here,
                we note that a dataset of size larger than 4 cannot be fit in the GPU we are using to do experiment. If time permits, we'll discuss the
                issue about image segmentation in the later section.
                </p>

                <p class="lead section-lead">2nd Attempt</p>

                <p class="section-paragraph">
                Here, we used mutliple reference images in order to generate a style image.
                </p>
                <center style="padding-bottom:35px;">
                    <img src="img/everfilter/6.jpg", width="40%">
                    <img src="img/everfilter/7.jpg", width="40%"><br>
                    <img src="img/everfilter/9.jpg", width="40%">
                    <img src="img/everfilter/15.jpg", width="40%"><br>
                    <img src="img/db4-everfilter6-7-9-15/everfilter6-7-9-15_at_iteration_599.png" width="80%"><br>
                    <div style="text-align:left;padding-left:10%; padding-right:10%;">
                    Figure 6: Another example of output from the algorithm for the dataset of size one. Top-left: the reference transformed image;
                    Top-right: the generated image after 15 epochs; Bottom-left: the generated image after 150 epochs; Bottom-right:
                    the generated image after 1500 epochs.
                    </div>
                </center>
                <center style="padding-bottom:35px;">
                    <iframe width="900" height="800" frameborder="0" scrolling="no" src="https://plot.ly/~mjenrungrot12f5/7.embed?share_key=twLhRfnmQU9EWYLvh60NvH"></iframe>
                    <div style="text-align:center;padding-left:10%; padding-right:10%;">
                    Figure 7: The learning curve for 3 examples shown above.
                    </div>
                </center>

                <p class="lead section-lead">3rd Attempt</p>
                <p class="section-paragraph">
                Here, we used a single reference image with masking to generate a style image.
                </p>

            </div>
        </div>
        <!-- /.row -->
    </div>
    <!-- /.container -->
</section>


<section>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1 class="section-heading">Conclusion</h1>
        <p class="section-paragraph">
            Using the definition of style according to <cite>Image style transfer using convolutional neural networks</cite> by using
            the Frobenius norm of difference between Gram matrices, we are able to see that the generated images will have similar color
            composition. Considering a generated image, one can see that its foreground portion and its background portion are mixed together.
            This stems from the definition of style we have. Then, we believe that this neural algorithm for style transfer can be improved if
            we can perform an image segmentation between a foreground portion and a background portion. This can perhaps produce a correct
            transformation between each portion of an image.
        </p>
      </div>
    </div>
    <!-- /.row -->
  </div>
  <!-- /.container -->
</section>


<section>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1 class="section-heading">Future Works</h1>
        <p class="section-paragraph">
            A recent paper <cite><a href="https://arxiv.org/pdf/1701.01036.pdf">Demystifying Neural Style Transfer</a></cite> explains that matching the Gram
        matrices of feature maps is equivalent to minimizing the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. The author
        further suggests that the style transfer problem is, essentially, equivalent to matching the feature distributions between the style images and the
        generated images. This finding agrees with our findings in this final project that generated style images seem to be able to pertain
        color composition of the reference transformed images. Hence, the process of using a Gram matrix to capture style information is equivalent to
        creating feature distribution for style in a certain image. We then believe that the work in the futures should incorporate different
        feature distributions for each portion of an image. As a feature distribution of background of an image can be different from a feature distribution
        of other parts of the same image, segmentating an image to different portion for creating its own feature distribution seems to be future researche
        </p>
      </div>
    </div>
    <!-- /.row -->
  </div>
  <!-- /.container -->
</section>

<section>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1 class="section-heading">Acknowledgements</h1>
        <p class="section-paragraph">
        This work used the Extreme Science and Engineering Discovery Environment (XSEDE),
         which is supported by National Science Foundation grant number OCI-1053575.
         Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606,
         at the Pittsburgh Supercomputing Center (PSC).”
        </p>
      </div>
    </div>
    <!-- /.row -->
  </div>
  <!-- /.container -->
</section>


<section>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1 class="section-heading">References</h1>
        <ol>
          <li>Chan, Ethan, and Rishabh Bhargava. "Show, Divide and Neural: Weighted Style Transfer". N.p., 2016. Web. 2017.</li>
          <li>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "A neural algorithm of artistic style." arXiv preprint arXiv:1508.06576 (2015).</li>
          <li>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "Image style transfer using convolutional neural networks." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.</li>
          <li>J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL
<a href="http://www.image-net.org/challenges/LSVRC/2012/">http://www.image-net.org/challenges/LSVRC/2012/</a>.</li>
          <li>Li, Yanghao, et al. "Demystifying Neural Style Transfer." arXiv preprint arXiv:1701.01036 (2017).</li>
          <li>Luan, Fujun, et al. "Deep Photo Style Transfer." arXiv preprint arXiv:1703.07511 (2017).</li>
          <li>Mahendran, Aravindh, and Andrea Vedaldi. "Understanding deep image representations by inverting them." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.</li>
          <li>Novak, Roman, and Yaroslav Nikulin. "Improving the neural algorithm of artistic style." arXiv preprint arXiv:1605.04603 (2016).</li>
          <li>Nystrom, N. A., Levine, M. J., Roskies, R. Z., and Scott, J. R. 2015. Bridges: A Uniquely Flexible HPC Resource for New Communities and Data Analytics. In Proceedings of the 2015 Annual Conference on Extreme Science and Engineering Discovery Environment (St. Louis, MO, July 26-30, 2015). XSEDE15. ACM, New York, NY, USA. http://dx.doi.org/10.1145/2792745.2792775.</li>
          <li>Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).</li>
          <li>Towns, J., Cockerill, T., Dahan, M., Foster, I., Gaither, K., Grimshaw, A., Hazlewood, V., Lathrop, S., Lifka, D., Peterson, G.D., Roskies, R., Scott, J.R. and Wilkens-Diehr, N. 2014. XSEDE: Accelerating Scientific Discovery. Computing in Science & Engineering. 16(5):62-74. http://doi.ieeecomputersociety.org/10.1109/MCSE.2014.80.</li>
          <li><a href="https://github.com/titu1994/Neural-Style-Transfer">https://github.com/titu1994/Neural-Style-Transfer</a></li>
          <li><a href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</a></li>
          </ol>
      </div>
    </div>
    <!-- /.row -->
  </div>
  <!-- /.container -->
</section>

<!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <p>Copyright &copy; Teerapat Jenrungrot 2017</p>
      </div>
    </div>
    <!-- /.row -->
  </div>
  <!-- /.container -->
</footer>

<!-- jQuery -->
<script src="js/jquery.js"></script>

<!-- Bootstrap Core JavaScript> -->
<script src="js/bootstrap.min.js"></script>

</body>

</html>