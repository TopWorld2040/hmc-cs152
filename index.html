<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>CS 152 - Final Project</title>

    <!-- - Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/full-width-pics.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script> 
    
    <!-- github button -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- font -->
    <link href='css/Source-Sans-Pro.css' rel='stylesheet' type='text/css'>
</head>

<body>
    <!-- Full Width Image Header with Logo -->
    <!-- Image backgrounds are set within the full-width-pics.css file. -->
    <header class="image-bg-fluid-height">
        <!-- <img class="img-responsive img-center" src="http://placehold.it/200x200&text=Logo" alt="">-->
    </header>

    <!-- Content Section -->
    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-9"></div>
                <div class="col-lg-1">
                    <a class="github-button" href="https://github.com/mjenrungrot/hmc-cs152/" data-icon="octicon-eye" data-style="mega" aria-label="Watch mjenrungrot/hmc-cs152 on GitHub">Watch</a>
                </div>
                <div class="col-lg-1">
                    <a class="github-button" href="https://github.com/mjenrungrot/hmc-cs152/fork" data-icon="octicon-repo-forked" data-style="mega" aria-label="Fork mjenrungrot/hmc-cs152 on GitHub">Fork</a>
                </div>
                <div class="col-lg-1">
                    <a class="github-button" href="https://github.com/mjenrungrot/hmc-cs152/archive/master.zip" data-icon="octicon-cloud-download" data-style="mega" aria-label="Download mjenrungrot/hmc-cs152 on GitHub">Download</a>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">CS152 - Neural Network Final Project</h1>
                    <h1 class="section-heading">Introduction</h1>
                    <p class="lead section-lead">Image Style Learning</p>
                    <p class="section-paragraph">
                        For the final project, I'm interested in an application of neural network on computer vision. <cite><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">
                        Image style transfer using convolutional neural networks</a></cite> proposes a neural algorithm for combining a content image and a
                        style image. The idea of this paper involves using an image representation derived from a convolutional neural network, as originated by <cite><a href="https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf">
                        Understanding deep image representations by inverting them</a></cite>. Using the similar idea of representing with a convolutional neural network, I'm interested in answer a question if we can create a neural algorithm for learning the transformation
                        between a content image and its transformed image. In other words, the scope of this final project is to focus mainly on generating a style image, given a content image and/or its transformed image. The outline of this webpage
                        will start by an introduction of previous works, an image representation, our proposed method, and we'll finish with the discussion, results, and future works.
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">Previous Works</h1>
                    <p class="lead section-lead"><cite>Image style transfer using convolutional neural networks</cite></p>
                    <p class="section-paragraph">
                        In transfering a style image to a content image problem, the goal is to synthesize a new image which contains a semantic content information of the content image and a style information of the reference style image. For example, one might be interested
                        in combining a content image depicting the Neckarfront in Tübinger, Germany, with <i>The Shipwreck of the Minotaur</i> by J.M.W. Turner, 1805. Figure 1 below shows such example.
                    </p>
                    <center>
                        <img src="img/example_style_transfer.png" width="85%"><br>
                        <div style="text-align:left;padding-left:10%; padding-right:10%;">
                            Figure 1: an example of result by a neural algorithm described in <cite>Image style transfer using convolutional neural networks</cite>. Left: a content image depicting the Neckarfront in Tuübinger, Germany; Right: a transformed
                            image generated by the algorithm; Bottom: the painting named <i>The Shipwreck of the Minotaur</i> by J.M.W. Turner, 1805. This example is taken from <cite>Image style transfer using convolutional neural networks</cite>.
                        </div>
                    </center>
                    <p class="section-paragraph">
                        Since the definition of style and content of an image can be misleading as we don't have definitive definitions for what contributes to a style or a content in an image, we then follow these two definitions according to <cite>Image style transfer using convolutional neural networks</cite>.
                        We define a few notations here: $\vec{p}$ represent a content image, $\vec{s}$ represent a style image, and
                        $\vec{x}$ represent an input image. Recall that the algorithm involves representing an image using the convolutional neural networks. To generalize, we define the set of 
                        layers from the network for a content image and a style image as $\mathcal{C}$ and $\mathcal{S}$ respectively.
                    </p>
                    <p class="section-paragraph">
                        <b>Definition 1:</b> Two images have similar content if their high-level features by the trained classifier have small Euclidean distance. In other words, the loss function regarding content representation of two images is
                    </p>
                        <center class="math-equation-center">
                            $\mathcal{L}_\text{content}(\vec{p},\vec{x}) = \sum\limits_{l\in\mathcal{C}} \left[ \frac{1}{U_l} \sum\limits_{i,j} \left(P_{ij}^{l} - X_{ij}^{l}\right)^2 \right]$
                        </div>
                    <p class="section-paragraph">
                        where $X_{ij}^l$ and $P_{ij}^{l}$ are the generated feature representations in layer $l$ of $i^\text{th}$ filter at position $j$ of the constructed image and a content image respectively. The term $U_l$ is the total number of units on layer $l$.
                    </p>
                    <p class="section-paragraph">
                        <b>Definition 2:</b> Two images have similar style if the difference between the features' Gram matrices has a small Frobenius norm. The Gram's matrix, $G_\vec{x}^l$, of an image $\vec{x}$ on layer $l$ is defined as follows:
                    </p>
                        <center class="math-equation-center">
                            $G_\vec{x}^l = \left[ G_{\vec{x},ij}^l \right]$
                        </center>
                    <p class="section-paragraph">
                        where each element of the Gram matrix is given by
                    </p>
                        <center class="math-equation-center">
                            $G_{\vec{x},ij}^{l} = \sum\limits_{k}X^{l}_{ik}X_{jk}^l$.
                        </center>
                    <p class="section-paragraph">
                        In the same way, we define the cost function representing the difference between two style images as follows:
                        <center class="math-equation-center">
                            $\mathcal{L}_\text{style}\left(\vec{s},\vec{x}\right) = \sum\limits_{l \in \mathcal{S}} \left[ \frac{1}{U_l} \sum\limits_{i,j}\left( G_{\vec{s},ij}^l - G_{\vec{x},ij}^l \right)^2 \right]$
                        </center>
                    </p>
                    <h4>Algorithm</h4>
                    <p class="section-paragraph">
                        In <cite>Image style transfer using convolutional neural networks</cite>, a deep convolutional neural network (VGG-19) is used. The set of layers for representing a content is 
                        $\mathcal{C} = \{\texttt{conv4_2}\}$. The set of layers for representing a style is $\mathcal{S} = \{\texttt{conv2_1}, \texttt{conv3_1}, \texttt{conv4_1}, \texttt{conv5_1}\}$. 
                        More information on the structure of VGG network is available at <cite><a href="https://arxiv.org/pdf/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></cite>.
                        The main advantages of using the VGG network, compared to other convolutional neural network, are 1) able to run on a large scale image recognition settings, 2) generalizable with different 
                        datasets, and 3) representable with deep networks. 
                    </p>
                    <p class="section-paragraph">
                        Note that the VGG network is the winner of the <a href="http://www.image-net.org/challenges/LSVRC/2012/">ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012)</a>. The dataset ImageNet contains over 10 millions of images with various kind of objects
                        We believe that the VGG network is a good candidate for encoding an image, as it's generalizable over many kinds of objects. The intuition for using a deep represenration of an image is that several layers in the convolutional neural networks
                        retain photographically accurate information about the image, with different degrees of geometric and photometric invariance. 
                    </p>
                    <p class="section-paragraph">
                        The algorithm starts by extracting content and style features and storing to be used. The style image $\vec{s}$ is then passed through the VGG-19 network, and we computed its style representation $G_{\vec{s}}^{l}$ on all layers $l \in \mathcal{S}.$
                        The content image $\vec{p}$ is passed through the VGG-19 network. Then, we obtained the content representation $P^{l}$ of all layers $l \in \mathcal{C}.$ Next, a random white noise image $\vec{x}$ is passed through the VGG-19 network. Its style features $G^{l}_{\vec{x}}$ and content features
                        $X^l$ on all layers are compated. On each layer included in the style representation layers $\mathcal{S}$ or the content representation layer $\mathcal{C}$, we compute the content loss $\mathcal{L}_\text{content}(\vec{p},\vec{x})$ and/or style loss $\mathcal{L}_\text{style}(\vec{s},\vec{x})$ accordingly.
                        We then used to optimize the total cost function that is defined by
                    </p>
                        <center class="math-equation-center">
                            $\mathcal{L}_\text{total}(\vec{p},\vec{s},\vec{x}) = \alpha \mathcal{L}_\text{content}(\vec{p},\vec{x}) + \beta \mathcal{L}_\text{style}(\vec{s},\vec{x})$
                        </center>
                        <p class="section-paragraph">
                     
                        where $\alpha$ and $\beta$ are weighting factors for content and style reconstruction, respectively. The learning algorithm will then optimize $\vec{x}$ such that it minimizes $\mathcal{L}_\text{total}(\vec{p},\vec{s},\vec{x})$ using gradient descent, such as L-BFGS.
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">Animated Style Learning</h1>
                    <p class="lead section-lead">Background</p>
                    <p class="section-paragraph">
                        In this final project, we aim to construct an algorithm that construct a style image 
                    </p>
                    <center>
                        <img src="img/example1_original.jpeg" width="42.5%">
                        <img src="img/example1_filtered.jpeg" width="42.5%"><br> 
                        <div style="text-align:center;padding-left:10%; padding-right:10%;">
                            Figure 2: Comparison between the original version and the expected version that the network will learn
                        </div>
                    </center>
                    <p class="lead section-lead">Approach</p>
                    <p class="section-paragraph">
                    In this section, we describe our approach on constructing a style image. We'll follow the definition of a similarity between two images' styles according to 
                    <cite>Image style transfer using convolutional neural networks</cite>. That is, two images have similar style if and only if the difference between
                    features' Gram matrices has a small Frobenius norm. Here, we re-modeled the objective function so that it excluded the cost function due to content loss.
                    The justification is that content information should take no effect in obtaining style information. Using a similar method of image reconstruction, we 
                    start with a randomized white noise and try to optimize the cost function using backpropagation to create a style image.
                    </p>
                    <p class="section-paragraph">
                    As the filter must be generalizable across various kind of images, we need to further construct a style image based on multiple transformed images. In this 
                    project, we sampled the transformed images manually from online sites. The objective function of our approach becomes
                    </p>
                        <center class="math-equation-center">
                            $\mathcal{L}(\texttt{dataset},\vec{x}) = \sum\limits_{i=1; \vec{s} \in \texttt{dataset}}^{|\texttt{dataset}|}\alpha_{i}\mathcal{L}_\text{style}(\vec{s},\vec{x})$
                        </center>
                        <p class="section-paragraph">
                     
                        where $\vec{s}$ and $\vec{x}$ is a style image vector and an input image vector, and $\alpha_i$ is a weighting parameter of each reference style image. 
                    </p>
                    <center>
                        <img src="img/proposed_algorithm.png" width=65%><br>
                        <div style="text-align:left;padding-left:10%; padding-right:10%;">
                        Figure 3: The structure of our proposed algorithm. At every iteration, we randomize images from the dataset, then input to the VGG-19 pre-trained network. 
                        We then calculate the Gram's matrix, F1, from the result of pre-trained network. To create a style image, we input to same VGG-19 pre-trained network in order to compute the Gram's matrix, F2, 
                        in the similar manner. We then update the style image using the back-propagation method to minimize the Frobenius norm between F1 and F2.
                        </div>
                    </center>
                    <p class="section-paragraph">
                    The workflow of our approach is depicted in Figure 3. We obtain the pre-trained model of VGG-19 model. Next, we applied the pre-trained VGG-19 model on 
                    all images in the dataset and store them to be used during optimization process. Then, during the optimization process, a white noise image gets passed
                    to the pre-trained VGG-19 model. Encodings on all layers of the pre-trained VGG-19 model are extracted to compute Gram's matrices, shown as F1 and F2 in 
                    Figure 3. Then, the objective function is calculated accordingly. The optimization process is an iterative algorithm and continues for a certain number
                    of iterations.
                    </p>

                    <p class="section-paragraph">
                    As one might notice in Figure 2 that the background is very different from the one in original picture, we suspect that Everfilter also did some sort
                    of image segmentation algorithm and applied the neural algorithm only on the foreground part of the image. To resolve this issue, we decided to use
                    to use a mask to differentiate information about an image's foreground and its background. We discuss this approach later in the next section.
                    </p>

                    <p class="lead section-lead">Results & Discussion</p>
                    <p class="section-paragraph">
                    In this project, we ran all the testings and optimization on the Pittsburgh Supercomputing Center, which is a high performance computing center. 
                    The specification of a GPU node I've been using is HPE Apollo 2000s, each with 2 NVIDIA P100 GPUs, 2 Intel Xeon E5-2683 v4 CPUs (16 cores per CPU), 
                    128GB RAM, and 8TB on-node storage. The NVIDIA P100 GPU has Memory Clock 1.4Gbps and VRAM 12GB.
                    </p>

                    <p class="section-paragraph">
                    First, we tested our approach with a dataset of size one. Figure 4, Figure 5, and Figure 6 show examples of output from our 
                    proposed algorithm. The training process takes roughly about 30 seconds for each epoch, which yields approximately 13 hours 
                    of running. Here, we include the generated images at different iterations.
                        <center>
                            <img src="img/everfilter/1.jpg", width="40%">
                            <img src="img/db1-everfilter1/everfilter1_at_iteration_15.png", width="40%"><br>
                            <img src="img/db1-everfilter1/everfilter1_at_iteration_150.png", width="40%">
                            <img src="img/db1-everfilter1/everfilter1_at_iteration_1500.png", width="40%">
                            <br>
                            <div style="text-align:left;padding-left:10%; padding-right:10%;">
                            Figure 4: An example of output from the algorithm for the dataset of size one. Top-left: the reference transformed image;
                            Top-right: the generated image after 15 epochs; Bottom-left: the generated image after 150 epochs; Bottom-right: 
                            the generated image after 1500 epochs. 
                            </div>
                        </center>
                    </p>
                    <center>
                            <img src="img/everfilter/13.jpg", width="40%">
                            <img src="img/db1-everfilter13/everfilter13_at_iteration_15.png", width="40%"><br>
                            <img src="img/db1-everfilter13/everfilter13_at_iteration_150.png", width="40%">
                            <img src="img/db1-everfilter13/everfilter13_at_iteration_1500.png", width="40%">
                            <br>
                            <div style="text-align:left;padding-left:10%; padding-right:10%;">
                            Figure 4: An example of output from the algorithm for the dataset of size one. Top-left: the reference transformed image;
                            Top-right: the generated image after 15 epochs; Bottom-left: the generated image after 150 epochs; Bottom-right: 
                            the generated image after 1500 epochs. 
                            </div>
                        </center>
                        <center>
                            <img src="img/everfilter/4.jpg", width="40%">
                            <img src="img/db1-everfilter4/everfilter4_at_iteration_15.png", width="40%"><br>
                            <img src="img/db1-everfilter4/everfilter4_at_iteration_150.png", width="40%">
                            <img src="img/db1-everfilter4/everfilter4_at_iteration_1500.png", width="40%">
                            <br>
                            <div style="text-align:left;padding-left:10%; padding-right:10%;">
                            Figure 4: An example of output from the algorithm for the dataset of size one. Top-left: the reference transformed image;
                            Top-right: the generated image after 15 epochs; Bottom-left: the generated image after 150 epochs; Bottom-right: 
                            the generated image after 1500 epochs. 
                            </div>
                        </center>
                    <p class="section-paragraph">
                    Figure 6 shows a learning curve of our algorithm.
                        <center>
                            <iframe width="900" height="800" frameborder="0" scrolling="no" src="https://plot.ly/~mjenrungrot12f5/7.embed?share_key=twLhRfnmQU9EWYLvh60NvH"></iframe>
                            <div style="text-align:left;">
                            Figure 5
                            </div>
                        </center>
                    </p>

                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">Future Works</h1>
                    <p class="section-paragraph">
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>


    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">References</h1>
                    <ol>
                        <li>Chan, Ethan, and Rishabh Bhargava. "Show, Divide and Neural: Weighted Style Transfer". N.p., 2016. Web. 2017.</li>
                        <li>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "A neural algorithm of artistic style." arXiv preprint arXiv:1508.06576 (2015).</li>
                        <li>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "Image style transfer using convolutional neural networks." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.</li>
                        <li>J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL
<a href="http://www.image-net.org/challenges/LSVRC/2012/">http://www.image-net.org/challenges/LSVRC/2012/</a>.</li>
                        <li>Li, Yanghao, et al. "Demystifying Neural Style Transfer." arXiv preprint arXiv:1701.01036 (2017).</li>
                        <li>Luan, Fujun, et al. "Deep Photo Style Transfer." arXiv preprint arXiv:1703.07511 (2017).</li>
                        <li>Mahendran, Aravindh, and Andrea Vedaldi. "Understanding deep image representations by inverting them." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.</li>
                        <li>Novak, Roman, and Yaroslav Nikulin. "Improving the neural algorithm of artistic style." arXiv preprint arXiv:1605.04603 (2016).</li>
                        <li>"Pittsburg Supercomputing Center Bridges User Guide." XSEDE User Portal | PSC Bridges User Guide. N.p., n.d. Web. 03 May 2017. <a href="https://portal.xsede.org/psc-bridges">https://portal.xsede.org/psc-bridges</a>.</li>
                        <li>Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).</li>
                        
                        </ol>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Teerapat Jenrungrot 2017</p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript> -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>