<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>CS 152 - Final Project</title>

    <!-- - Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/full-width-pics.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!-- font -->
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro' rel='stylesheet' type='text/css'>
</head>

<body>
    <!-- Full Width Image Header with Logo -->
    <!-- Image backgrounds are set within the full-width-pics.css file. -->
    <header class="image-bg-fluid-height">
        <!-- <img class="img-responsive img-center" src="http://placehold.it/200x200&text=Logo" alt="">-->
    </header>

    <!-- Content Section -->
    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">CS152 - Neural Network Final Project</h1>
                    <h1 class="section-heading">Introduction</h1>
                    <p class="lead section-lead">Image Style Learning</p>
                    <p class="section-paragraph">
                        For the final project, I'm interested in an application of neural network on computer vision. <cite>Image style transfer using convolutional neural networks</cite> proposes a neural algorithm for combining a content image and a
                        style image. The idea of this paper involves using an image representation derived from a convolutional neural network, as originated by <cite>
                Understanding deep image representations by inverting them</cite>. Using the similar idea of representing with a convolutional neural network, I'm interested in answer a question if we can create a neural algorithm for learning the transformation
                        between a content image and its transformed image. In other words, the scope of this final project is to focus mainly on generating a style image, given a content image and/or its transformed image. The outline of this webpage
                        will start by an introduction of previous works, an image representation, our proposed method, and we'll finish with the discussion, results, and future works.
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">Previous Works</h1>
                    <p class="lead section-lead"><cite>Image style transfer using convolutional neural networks</cite></p>
                    <p class="section-paragraph">
                        In transfering a style image to a content image problem, the goal is to synthesize a new image which contains a semantic content information of the content image and a style information of the reference style image. For example, one might be interested
                        in combining a content image depicting the Neckarfront in Tübinger, Germany, with <i>The Shipwreck of the Minotaur</i> by J.M.W. Turner, 1805. Figure 1 below shows such example.
                    </p>
                    <center>
                        <img src="img/example_style_transfer.png" width="85%"><br>
                        <div style="text-align:left;">
                            Figure 1: an example of result by a neural algorithm described in <cite>Image style transfer using convolutional neural networks</cite>. Left: a content image depicting the Neckarfront in Tuübinger, Germany; Right: a transformed
                            image generated by the algorithm; Bottom: the paining named <i>The Shipwreck of the Minotaur</i> by J.M.W. Turner, 1805. This example is taken from <cite>Image style transfer using convolutional neural networks</cite>.
                        </div>
                    </center>
                    <p class="section-paragraph">
                        Since the definition of style and content of an image can be misleading as we don't have definitive definitions for what contributes to a style or a content in an image, we then follow these two definitions according to <cite>Image style transfer using convolutional neural networks</cite>.
                        We define a few notations here: $\vec{p}$ represent a content image, $\vec{s}$ represent a style image, and
                        $\vec{x}$ represent an input image. Recall that the algorithm involves representing an image using the convolutional neural networks. To generalize, we define the set of 
                        layers from the network for a content image and a style image as $\mathcal{C}$ and $\mathcal{S}$ respectively.
                    </p>
                    <p class="section-paragraph">
                        <b>Definition 1:</b> Two images have similar content if their high-level features by the trained classifier have small Euclidean distance. In other words, the loss function regarding content representation of two images is
                        <center>
                            $\mathcal{L}_\text{content}(\vec{p},\vec{x}) = \sum\limits_{l\in\mathcal{C}} \left[ \frac{1}{U_l} \sum\limits_{i,j} \left(P_{ij}^{l} - X_{ij}^{l}\right)^2 \right]$
                        </center>
                        where $X_{ij}^l$ and $P_{ij}^{l}$ are the generated feature representations in layer $l$ of $i^\text{th}$ filter at position $j$ of the constructed image and a content image respectively. The term $U_l$ is the total number of units on layer $l$.
                    </p>
                    <p class="section-paragraph">
                        <b>Definition 2:</b> Two images have similar style if the difference between the features' Gram matrices has a small Frobenius norm. The Gram's matrix, $G_\vec{x}^l$, of an image $\vec{x}$ on layer $l$ is defined as follows:
                        <center>
                            $G_\vec{x}^l = \left[ G_{\vec{x},ij}^l \right]$
                        </center>
                        where each element of the Gram matrix is given by
                        <center>
                            $G_{\vec{x},ij}^{l} = \sum\limits_{k}X^{l}_{ik}X_{jk}^l$.
                        </center>
                        In the same way, we define the cost function representing the difference between two style images as follows:
                        <center>
                            $\mathcal{L}_\text{style}\left(\vec{s},\vec{x}\right) = \sum\limits_{l \in \mathcal{S}} \left[ \frac{1}{U_l} \sum\limits_{i,j}\left( G_{\vec{s},ij}^l - G_{\vec{x},ij}^l \right)^2 \right]$
                        </center>
                    </p>
                    <h4>Algorithm</h4>
                    <p class="section-paragraph">
                        In <cite>Image style transfer using convolutional neural networks</cite>, a deep convolutional neural network (VGG-19) is used. The set of layers for representing a content is 
                        $\mathcal{C} = \{\texttt{conv4_2}\}$. The set of layers for representing a style is $\mathcal{S} = \{\texttt{conv2_1}, \texttt{conv3_1}, \texttt{conv4_1}, \texttt{conv5_1}\}$. 
                        More information on the structure of VGG network is available at <cite><a href="https://arxiv.org/pdf/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></cite>.
                        The main advantages of using the VGG network, compared to other convolutional neural network, are 1) able to run on a large scale image recognition settings, 2) generalizable with different 
                        datasets, and 3) representable with deep networks. 
                    </p>
                    <p class="section-paragraph">
                        Note that the VGG network is the winner of the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The dataset ImageNet contains over 10 millions of images with various kind of objects
                        We believe that the VGG network is a good candidate for encoding an image, as it's generalizable over many kinds of objects. The intuition for using a deep represenration of an image is that several layers in the convolutional neural networks
                        retain photographically accurate information about the image, with different degrees of geometric and photometric invariance. 
                    </p>
                    <p class="section-paragraph">
                        The algorithm starts by extracting content and style features and storing to be used. The style image $\vec{s}$ is then passed through the VGG-19 network, and we computed its style representation $G_{\vec{s}}^{l}$ on all layers $l \in \mathcal{S}.$
                        The content image $\vec{p}$ is passed through the VGG-19 network. Then, we obtained the content representation $P^{l}$ of all layers $l \in \mathcal{C}.$ Next, a random white noise image $\vec{x}$ is passed through the VGG-19 network. Its style features $G^{l}_{\vec{x}}$ and content features
                        $X^l$ on all layers are compated. On each layer included in the style representation layers $\mathcal{S}$ or the content representation layer $\mathcal{C}$, we compute the content loss $\mathcal{L}_\text{content}(\vec{p},\vec{x})$ and/or style loss $\mathcal{L}_\text{style}(\vec{s},\vec{x})$ accordingly.
                        We then used to optimize the total cost function that is defined by
                        <center>
                            $\mathcal{L}_\text{total}(\vec{p},\vec{s},\vec{x}) = \alpha \mathcal{L}_\text{content}(\vec{p},\vec{x}) + \beta \mathcal{L}_\text{style}(\vec{s},\vec{x})$
                        </center>
                        where $\alpha$ and $\beta$ are weighting factors for content and style reconstruction, respectively. The learning algorithm will then optimize $\vec{x}$ such that it minimizes $\mathcal{L}_\text{total}(\vec{p},\vec{s},\vec{x})$ using gradient descent, such as L-BFGS.
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">Animated Style Learning</h1>
                    <p class="lead section-lead">Background</p>
                    <p class="section-paragraph">
                        In this final project, we aim to construct an algorithm that construct a style image 
                    </p>
                    <center>
                        <img src="img/example1_original.jpeg" width="42.5%">
                        <img src="img/example1_filtered.jpeg" width="42.5%"><br> Figure 2 & 3: Comparison between the original version and the expected version that the network will learn
                    </center>
                    <p class="section-paragraph">
                        From inspection, we can see that the content of images remains almost the same; however, the background content changes siginificantly. This suggests that the system we are trying to create has to be able to separate the background and the content. There
                        are 3 main papers in the fields that are seemingly strongly related to this issue:
                    </p>
                    <p class="lead section-lead">Approach</p>
                    <p class="section-paragraph">
                    In this section, we describe the approach we used to answer our problem of finding a style image. Recall that the neural algorithm of style transfer \cite{gatys2016image} starts from a white noise for the generated image. 
                    We propose to do in the similar manner by initializing the style image from white noise, and we will keep updating the style image by using back propagation to minimize the objective function. In our proposed approach, 
                    we will use the same objective function as \cite{gatys2016image}; however, we'll ignore the content loss. 
                    </p>
                    <p class="section-paragraph">
                    We first generated an image dataset of EverFilter transformed images. First, we make two assumptions, first, that the EverFilter transformed images are created by similar algorithm as the neural algorithm for the style transfer and, 
                    second, that the style image used to create these transformed images is the same.
                    </p>
                    <p class="section-paragraph">
                        According to <cite>Image style transfer using convolutional neural networks</cite>'s definition of style similarity, two images have similar style if and only if the difference between the features' Gram matrices has a small Frobenius norm. 
                        We proposed the structure of algorithm as shown in Figure 4.
                    </p>
                    <center>
                        <img src="img/proposed_algorithm.png" width=85%><br>
                        <div style="text-align:left;">
                        Figure 4: The structure of our proposed algorithm. At every iteration, we randomize an image from the dataset, then input to the VGG-19 pre-trained network. 
                        We then calculate the Gram's matrix, F1, from the result of pre-trained network. To create a style image, we input to same VGG-19 pre-trained network in order to compute the Gram's matrix, F2, 
                        in the similar manner. We then update the style image using the back-propagation method to minimize the Frobenius norm between F1 and F2.
                        </div>
                    </center>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">Future Works</h1>
                    <p class="section-paragraph">
                    </p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>


    <section>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1 class="section-heading">References</h1>
                    <ol>
                        <li>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "A neural algorithm of artistic style." arXiv preprint arXiv:1508.06576 (2015).</li>
                        <li>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "Image style transfer using convolutional neural networks." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.</li>
                    </ol>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Teerapat Jenrungrot 2017</p>
                </div>
            </div>
            <!-- /.row -->
        </div>
        <!-- /.container -->
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript> -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>